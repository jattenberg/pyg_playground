{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e23e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0196ec4",
   "metadata": {},
   "source": [
    "### Cora Data Set\n",
    "see https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d88961",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='.', name='Cora')\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=True,\n",
    "                             normalize=True)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels, cached=True,\n",
    "                             normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "model = GCN(dataset.num_features, 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa0da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(1433, 16)\n",
       "  (conv2): GCNConv(16, 7)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ea1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = model.to(device), data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c202f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "    dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "], lr=0.01)  # Only perform weight-decay on first convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe64cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6639a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index, data.edge_weight).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d68f1234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1, Loss=0.03572746738791466, Train=1.0, Val=0.764, Test=0.798\n",
      "Epoch=2, Loss=0.027080627158284187, Train=1.0, Val=0.766, Test=0.795\n",
      "Epoch=3, Loss=0.054363250732421875, Train=1.0, Val=0.764, Test=0.795\n",
      "Epoch=4, Loss=0.04991898313164711, Train=1.0, Val=0.766, Test=0.795\n",
      "Epoch=5, Loss=0.05219227075576782, Train=1.0, Val=0.768, Test=0.8\n",
      "Epoch=6, Loss=0.04436289146542549, Train=1.0, Val=0.77, Test=0.803\n",
      "Epoch=7, Loss=0.02423805743455887, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=8, Loss=0.0315953828394413, Train=1.0, Val=0.774, Test=0.801\n",
      "Epoch=9, Loss=0.05530337244272232, Train=1.0, Val=0.774, Test=0.801\n",
      "Epoch=10, Loss=0.023088684305548668, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=11, Loss=0.03182704746723175, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=12, Loss=0.04353439062833786, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=13, Loss=0.04713058099150658, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=14, Loss=0.046349626034498215, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=15, Loss=0.0391673818230629, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=16, Loss=0.028324395418167114, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=17, Loss=0.04816403239965439, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=18, Loss=0.038828857243061066, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=19, Loss=0.02738240361213684, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=20, Loss=0.04802584648132324, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=21, Loss=0.02982240729033947, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=22, Loss=0.08263056725263596, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=23, Loss=0.04450474679470062, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=24, Loss=0.034286968410015106, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=25, Loss=0.026130327954888344, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=26, Loss=0.030384710058569908, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=27, Loss=0.025316495448350906, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=28, Loss=0.034943837672472, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=29, Loss=0.02504916302859783, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=30, Loss=0.07176188379526138, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=31, Loss=0.028131039813160896, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=32, Loss=0.0290200337767601, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=33, Loss=0.020296970382332802, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=34, Loss=0.01682763360440731, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=35, Loss=0.060259751975536346, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=36, Loss=0.036133792251348495, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=37, Loss=0.0422288253903389, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=38, Loss=0.027985190972685814, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=39, Loss=0.04096705839037895, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=40, Loss=0.046190064400434494, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=41, Loss=0.056463103741407394, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=42, Loss=0.03769737854599953, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=43, Loss=0.04407047852873802, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=44, Loss=0.041953545063734055, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=45, Loss=0.0394950695335865, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=46, Loss=0.05420888587832451, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=47, Loss=0.03691054880619049, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=48, Loss=0.04082654416561127, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=49, Loss=0.03434446454048157, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=50, Loss=0.023257244378328323, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=51, Loss=0.03645977005362511, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=52, Loss=0.023169944062829018, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=53, Loss=0.036672163754701614, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=54, Loss=0.04700149968266487, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=55, Loss=0.024759482592344284, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=56, Loss=0.08084297180175781, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=57, Loss=0.05732445791363716, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=58, Loss=0.021730883046984673, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=59, Loss=0.023422906175255775, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=60, Loss=0.024413662031292915, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=61, Loss=0.028169194236397743, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=62, Loss=0.035006992518901825, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=63, Loss=0.033198993653059006, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=64, Loss=0.01793188974261284, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=65, Loss=0.027724502608180046, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=66, Loss=0.017779570072889328, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=67, Loss=0.030948318541049957, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=68, Loss=0.03364281356334686, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=69, Loss=0.03428804874420166, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=70, Loss=0.023216433823108673, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=71, Loss=0.024367600679397583, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=72, Loss=0.020980242639780045, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=73, Loss=0.027410166338086128, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=74, Loss=0.057369641959667206, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=75, Loss=0.026964295655488968, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=76, Loss=0.03568791598081589, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=77, Loss=0.03688674420118332, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=78, Loss=0.06382051855325699, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=79, Loss=0.05357992276549339, Train=1.0, Val=0.748, Test=0.804\n",
      "Epoch=80, Loss=0.029881859198212624, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=81, Loss=0.03376677632331848, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=82, Loss=0.03568180277943611, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=83, Loss=0.026093676686286926, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=84, Loss=0.027737289667129517, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=85, Loss=0.02371087670326233, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=86, Loss=0.019812216982245445, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=87, Loss=0.03903456777334213, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=88, Loss=0.03802083805203438, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=89, Loss=0.03625939041376114, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=90, Loss=0.021564435213804245, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=91, Loss=0.050749193876981735, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=92, Loss=0.02949672006070614, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=93, Loss=0.017707999795675278, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=94, Loss=0.06807629019021988, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=95, Loss=0.03887046501040459, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=96, Loss=0.02653707191348076, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=97, Loss=0.031017694622278214, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=98, Loss=0.03372308239340782, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=99, Loss=0.02222260646522045, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=100, Loss=0.04196183755993843, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=101, Loss=0.04443598911166191, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=102, Loss=0.02532370202243328, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=103, Loss=0.03526560217142105, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=104, Loss=0.022071363404393196, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=105, Loss=0.05101925507187843, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=106, Loss=0.01063208281993866, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=107, Loss=0.06862223893404007, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=108, Loss=0.03180366009473801, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=109, Loss=0.027012286707758904, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=110, Loss=0.023336447775363922, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=111, Loss=0.024482009932398796, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=112, Loss=0.027848470956087112, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=113, Loss=0.028864411637187004, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=114, Loss=0.017086759209632874, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=115, Loss=0.032299820333719254, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=116, Loss=0.015982020646333694, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=117, Loss=0.05227961018681526, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=118, Loss=0.05983183905482292, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=119, Loss=0.02882125973701477, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=120, Loss=0.025664230808615685, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=121, Loss=0.026708772405982018, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=122, Loss=0.035098303109407425, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=123, Loss=0.02237199991941452, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=124, Loss=0.01727220043540001, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=125, Loss=0.02132529579102993, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=126, Loss=0.057280976325273514, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=127, Loss=0.034998033195734024, Train=1.0, Val=0.758, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=128, Loss=0.02938162535429001, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=129, Loss=0.030716337263584137, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=130, Loss=0.021251151338219643, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=131, Loss=0.021193480119109154, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=132, Loss=0.04299963265657425, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=133, Loss=0.02870912291109562, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=134, Loss=0.020981866866350174, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=135, Loss=0.016179589554667473, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=136, Loss=0.026384038850665092, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=137, Loss=0.03094840794801712, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=138, Loss=0.026016319170594215, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=139, Loss=0.022624244913458824, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=140, Loss=0.04215715825557709, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=141, Loss=0.017967654392123222, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=142, Loss=0.02140236459672451, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=143, Loss=0.05812396854162216, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=144, Loss=0.04201784357428551, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=145, Loss=0.023006413131952286, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=146, Loss=0.03186151757836342, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=147, Loss=0.02146846614778042, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=148, Loss=0.019969230517745018, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=149, Loss=0.02489764243364334, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=150, Loss=0.042091794312000275, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=151, Loss=0.015539218671619892, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=152, Loss=0.028850987553596497, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=153, Loss=0.0489346869289875, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=154, Loss=0.03259486332535744, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=155, Loss=0.029485534876585007, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=156, Loss=0.04008078575134277, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=157, Loss=0.029783358797430992, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=158, Loss=0.03847910836338997, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=159, Loss=0.04051372408866882, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=160, Loss=0.03122732974588871, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=161, Loss=0.04292815923690796, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=162, Loss=0.02107805572450161, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=163, Loss=0.025148408487439156, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=164, Loss=0.029373452067375183, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=165, Loss=0.025630414485931396, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=166, Loss=0.024188591167330742, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=167, Loss=0.024601450189948082, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=168, Loss=0.030743364244699478, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=169, Loss=0.02221464365720749, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=170, Loss=0.03528439998626709, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=171, Loss=0.02642778307199478, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=172, Loss=0.03944981470704079, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=173, Loss=0.01684325747191906, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=174, Loss=0.017280826345086098, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=175, Loss=0.02062820829451084, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=176, Loss=0.036499883979558945, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=177, Loss=0.03353936970233917, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=178, Loss=0.013261249288916588, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=179, Loss=0.03271952643990517, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=180, Loss=0.019056666642427444, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=181, Loss=0.051548440009355545, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=182, Loss=0.028408033773303032, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=183, Loss=0.028846457600593567, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=184, Loss=0.03285849839448929, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=185, Loss=0.021406957879662514, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=186, Loss=0.042662423104047775, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=187, Loss=0.03917704150080681, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=188, Loss=0.03111681528389454, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=189, Loss=0.04458210989832878, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=190, Loss=0.0313616618514061, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=191, Loss=0.03173409774899483, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=192, Loss=0.03985324874520302, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=193, Loss=0.02147790417075157, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=194, Loss=0.029279634356498718, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=195, Loss=0.015797404572367668, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=196, Loss=0.012289738282561302, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=197, Loss=0.03141840919852257, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=198, Loss=0.034867845475673676, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=199, Loss=0.022177746519446373, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=200, Loss=0.03663703426718712, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=201, Loss=0.01220451295375824, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=202, Loss=0.020016774535179138, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=203, Loss=0.016806084662675858, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=204, Loss=0.03052658773958683, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=205, Loss=0.026240292936563492, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=206, Loss=0.03672889992594719, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=207, Loss=0.021402793005108833, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=208, Loss=0.01585063897073269, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=209, Loss=0.02245817333459854, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=210, Loss=0.020844193175435066, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=211, Loss=0.02858731709420681, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=212, Loss=0.03454359620809555, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=213, Loss=0.02980753406882286, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=214, Loss=0.03720025718212128, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=215, Loss=0.023414893075823784, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=216, Loss=0.018520032986998558, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=217, Loss=0.027492178604006767, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=218, Loss=0.04284197837114334, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=219, Loss=0.029914531856775284, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=220, Loss=0.04941945523023605, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=221, Loss=0.02022089622914791, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=222, Loss=0.031776830554008484, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=223, Loss=0.018084490671753883, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=224, Loss=0.043040815740823746, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=225, Loss=0.023870574310421944, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=226, Loss=0.05374085530638695, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=227, Loss=0.022081952542066574, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=228, Loss=0.01906171627342701, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=229, Loss=0.0361979641020298, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=230, Loss=0.02269144169986248, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=231, Loss=0.03125970438122749, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=232, Loss=0.023432040587067604, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=233, Loss=0.07002376765012741, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=234, Loss=0.019178401678800583, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=235, Loss=0.03940483182668686, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=236, Loss=0.02283550426363945, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=237, Loss=0.02153571881353855, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=238, Loss=0.04561535641551018, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=239, Loss=0.0462944433093071, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=240, Loss=0.05045640468597412, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=241, Loss=0.03144009783864021, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=242, Loss=0.023045450448989868, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=243, Loss=0.020736416801810265, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=244, Loss=0.025103898718953133, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=245, Loss=0.013515734113752842, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=246, Loss=0.02091514691710472, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=247, Loss=0.03390435501933098, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=248, Loss=0.03536256402730942, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=249, Loss=0.029438870027661324, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=250, Loss=0.027879251167178154, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=251, Loss=0.017628494650125504, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=252, Loss=0.026473168283700943, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=253, Loss=0.040345676243305206, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=254, Loss=0.02048739604651928, Train=1.0, Val=0.77, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=255, Loss=0.0607202872633934, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=256, Loss=0.025865629315376282, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=257, Loss=0.02914176695048809, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=258, Loss=0.026154903694987297, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=259, Loss=0.040770016610622406, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=260, Loss=0.04886345937848091, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=261, Loss=0.024148600175976753, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=262, Loss=0.022515039891004562, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=263, Loss=0.027237530797719955, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=264, Loss=0.02138066664338112, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=265, Loss=0.025189444422721863, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=266, Loss=0.022241294384002686, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=267, Loss=0.02186308056116104, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=268, Loss=0.030065208673477173, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=269, Loss=0.024141475558280945, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=270, Loss=0.026700224727392197, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=271, Loss=0.016903849318623543, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=272, Loss=0.017617085948586464, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=273, Loss=0.020671404898166656, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=274, Loss=0.050880756229162216, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=275, Loss=0.05653880909085274, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=276, Loss=0.0403401255607605, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=277, Loss=0.010647517628967762, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=278, Loss=0.03646811842918396, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=279, Loss=0.015967538580298424, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=280, Loss=0.02364361844956875, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=281, Loss=0.03317045420408249, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=282, Loss=0.025360852479934692, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=283, Loss=0.0433792807161808, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=284, Loss=0.032390058040618896, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=285, Loss=0.024752018973231316, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=286, Loss=0.014222413301467896, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=287, Loss=0.015599831938743591, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=288, Loss=0.027799563482403755, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=289, Loss=0.034943439066410065, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=290, Loss=0.0199716966599226, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=291, Loss=0.03280255198478699, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=292, Loss=0.01995539851486683, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=293, Loss=0.026611778885126114, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=294, Loss=0.027785440906882286, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=295, Loss=0.025887610390782356, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=296, Loss=0.02110564149916172, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=297, Loss=0.0314238965511322, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=298, Loss=0.021489808335900307, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=299, Loss=0.021930955350399017, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=300, Loss=0.02171788364648819, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=301, Loss=0.015046541579067707, Train=1.0, Val=0.74, Test=0.807\n",
      "Epoch=302, Loss=0.03157156705856323, Train=1.0, Val=0.738, Test=0.807\n",
      "Epoch=303, Loss=0.01838449202477932, Train=1.0, Val=0.74, Test=0.807\n",
      "Epoch=304, Loss=0.015644755214452744, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=305, Loss=0.015214190818369389, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=306, Loss=0.021709149703383446, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=307, Loss=0.018917996436357498, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=308, Loss=0.03478388115763664, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=309, Loss=0.027741244062781334, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=310, Loss=0.03150385990738869, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=311, Loss=0.041763100773096085, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=312, Loss=0.031263068318367004, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=313, Loss=0.0372210331261158, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=314, Loss=0.013969458639621735, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=315, Loss=0.024100396782159805, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=316, Loss=0.030104998499155045, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=317, Loss=0.02999335341155529, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=318, Loss=0.02580263838171959, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=319, Loss=0.04192299768328667, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=320, Loss=0.030103649944067, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=321, Loss=0.02082606591284275, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=322, Loss=0.014410141855478287, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=323, Loss=0.025629544630646706, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=324, Loss=0.05671802535653114, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=325, Loss=0.019914772361516953, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=326, Loss=0.02211288921535015, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=327, Loss=0.026399049907922745, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=328, Loss=0.0230729877948761, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=329, Loss=0.025928858667612076, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=330, Loss=0.024508092552423477, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=331, Loss=0.017034443095326424, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=332, Loss=0.01853514276444912, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=333, Loss=0.028566136956214905, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=334, Loss=0.025287380442023277, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=335, Loss=0.020788850262761116, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=336, Loss=0.023379432037472725, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=337, Loss=0.027836933732032776, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=338, Loss=0.02918243780732155, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=339, Loss=0.026260187849402428, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=340, Loss=0.029549553990364075, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=341, Loss=0.03792151063680649, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=342, Loss=0.034285224974155426, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=343, Loss=0.02278115786612034, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=344, Loss=0.022182518616318703, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=345, Loss=0.027699509635567665, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=346, Loss=0.019858678802847862, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=347, Loss=0.018311139196157455, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=348, Loss=0.0158003568649292, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=349, Loss=0.019750820472836494, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=350, Loss=0.01696627587080002, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=351, Loss=0.05800798907876015, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=352, Loss=0.02312544733285904, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=353, Loss=0.02060052752494812, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=354, Loss=0.036635417491197586, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=355, Loss=0.029960427433252335, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=356, Loss=0.023276954889297485, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=357, Loss=0.032138243317604065, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=358, Loss=0.01543128676712513, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=359, Loss=0.014393038116395473, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=360, Loss=0.027124671265482903, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=361, Loss=0.03291940316557884, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=362, Loss=0.03051827661693096, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=363, Loss=0.015374847687780857, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=364, Loss=0.017975546419620514, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=365, Loss=0.026021361351013184, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=366, Loss=0.013760206289589405, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=367, Loss=0.04978413134813309, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=368, Loss=0.015324427746236324, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=369, Loss=0.04635753482580185, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=370, Loss=0.02608191780745983, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=371, Loss=0.017385408282279968, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=372, Loss=0.02110430784523487, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=373, Loss=0.021638020873069763, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=374, Loss=0.037057261914014816, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=375, Loss=0.03392726555466652, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=376, Loss=0.039220210164785385, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=377, Loss=0.010242527350783348, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=378, Loss=0.01455006469041109, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=379, Loss=0.03398555889725685, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=380, Loss=0.022160619497299194, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=381, Loss=0.022386593744158745, Train=1.0, Val=0.746, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=382, Loss=0.02873491495847702, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=383, Loss=0.02155459113419056, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=384, Loss=0.020460888743400574, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=385, Loss=0.026946336030960083, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=386, Loss=0.015713194385170937, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=387, Loss=0.024823546409606934, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=388, Loss=0.023833325132727623, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=389, Loss=0.04478349909186363, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=390, Loss=0.03244169056415558, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=391, Loss=0.01292932964861393, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=392, Loss=0.013050869107246399, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=393, Loss=0.019755607470870018, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=394, Loss=0.012502972967922688, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=395, Loss=0.020172564312815666, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=396, Loss=0.008623613975942135, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=397, Loss=0.019454726949334145, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=398, Loss=0.027375413104891777, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=399, Loss=0.01885213889181614, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=400, Loss=0.018625447526574135, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=401, Loss=0.03925570473074913, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=402, Loss=0.0099246297031641, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=403, Loss=0.012111794203519821, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=404, Loss=0.03053538128733635, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=405, Loss=0.026986172422766685, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=406, Loss=0.018059462308883667, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=407, Loss=0.02036176808178425, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=408, Loss=0.028305111452937126, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=409, Loss=0.024492857977747917, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=410, Loss=0.0268862284719944, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=411, Loss=0.018480004742741585, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=412, Loss=0.03640947863459587, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=413, Loss=0.022152753546833992, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=414, Loss=0.017484726384282112, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=415, Loss=0.031449977308511734, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=416, Loss=0.012034684419631958, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=417, Loss=0.02615722082555294, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=418, Loss=0.017489507794380188, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=419, Loss=0.03317170962691307, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=420, Loss=0.027340469881892204, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=421, Loss=0.019080284982919693, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=422, Loss=0.02282104641199112, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=423, Loss=0.01535126008093357, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=424, Loss=0.017443779855966568, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=425, Loss=0.03643113747239113, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=426, Loss=0.018708471208810806, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=427, Loss=0.023199403658509254, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=428, Loss=0.016505764797329903, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=429, Loss=0.03073250874876976, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=430, Loss=0.033446986228227615, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=431, Loss=0.024500971660017967, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=432, Loss=0.011346369050443172, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=433, Loss=0.02511567249894142, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=434, Loss=0.023361513391137123, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=435, Loss=0.01621869206428528, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=436, Loss=0.028529459610581398, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=437, Loss=0.022468766197562218, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=438, Loss=0.016094094142317772, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=439, Loss=0.030012179166078568, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=440, Loss=0.031625233590602875, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=441, Loss=0.02383890002965927, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=442, Loss=0.037728238850831985, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=443, Loss=0.024115124717354774, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=444, Loss=0.014211314730346203, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=445, Loss=0.01788743957877159, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=446, Loss=0.02856701798737049, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=447, Loss=0.03994261473417282, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=448, Loss=0.018572816625237465, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=449, Loss=0.02078954689204693, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=450, Loss=0.021139483898878098, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=451, Loss=0.027252955362200737, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=452, Loss=0.025782672688364983, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=453, Loss=0.034890592098236084, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=454, Loss=0.016420792788267136, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=455, Loss=0.03970080986618996, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=456, Loss=0.028638560324907303, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=457, Loss=0.043367791920900345, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=458, Loss=0.015786288306117058, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=459, Loss=0.022068902850151062, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=460, Loss=0.015664905309677124, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=461, Loss=0.02389582060277462, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=462, Loss=0.024946585297584534, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=463, Loss=0.020903371274471283, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=464, Loss=0.011955772526562214, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=465, Loss=0.012673077173531055, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=466, Loss=0.024926956743001938, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=467, Loss=0.006673313677310944, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=468, Loss=0.013706585392355919, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=469, Loss=0.018341531977057457, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=470, Loss=0.01542400848120451, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=471, Loss=0.013566359877586365, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=472, Loss=0.03221401572227478, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=473, Loss=0.01636802777647972, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=474, Loss=0.019165905192494392, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=475, Loss=0.03335732966661453, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=476, Loss=0.031886227428913116, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=477, Loss=0.03605452924966812, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=478, Loss=0.017764052376151085, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=479, Loss=0.015547283925116062, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=480, Loss=0.03067396767437458, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=481, Loss=0.02973290905356407, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=482, Loss=0.049738235771656036, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=483, Loss=0.044424477964639664, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=484, Loss=0.03131238371133804, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=485, Loss=0.023735564202070236, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=486, Loss=0.01857234537601471, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=487, Loss=0.012835576198995113, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=488, Loss=0.02123422548174858, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=489, Loss=0.04912915080785751, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=490, Loss=0.01921672560274601, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=491, Loss=0.020250553265213966, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=492, Loss=0.021583620458841324, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=493, Loss=0.023118823766708374, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=494, Loss=0.017183292657136917, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=495, Loss=0.03257168456912041, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=496, Loss=0.028839386999607086, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=497, Loss=0.02149159274995327, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=498, Loss=0.017275173217058182, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=499, Loss=0.018159879371523857, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=500, Loss=0.027787892147898674, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=501, Loss=0.05267728120088577, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=502, Loss=0.019599799066781998, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=503, Loss=0.0224883034825325, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=504, Loss=0.020600758492946625, Train=1.0, Val=0.764, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=505, Loss=0.02098066359758377, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=506, Loss=0.02836127020418644, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=507, Loss=0.015201235190033913, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=508, Loss=0.038573283702135086, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=509, Loss=0.02880648523569107, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=510, Loss=0.014180236496031284, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=511, Loss=0.016018101945519447, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=512, Loss=0.019505048170685768, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=513, Loss=0.017711691558361053, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=514, Loss=0.03764842823147774, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=515, Loss=0.012526057660579681, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=516, Loss=0.03838512673974037, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=517, Loss=0.02671477198600769, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=518, Loss=0.021533746272325516, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=519, Loss=0.02135302498936653, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=520, Loss=0.017788343131542206, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=521, Loss=0.019047096371650696, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=522, Loss=0.03668566048145294, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=523, Loss=0.036256689578294754, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=524, Loss=0.020771970972418785, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=525, Loss=0.02494182251393795, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=526, Loss=0.014219370670616627, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=527, Loss=0.012373294681310654, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=528, Loss=0.02189745381474495, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=529, Loss=0.01824679784476757, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=530, Loss=0.015068101696670055, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=531, Loss=0.020005570724606514, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=532, Loss=0.013471479527652264, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=533, Loss=0.038924116641283035, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=534, Loss=0.02074311673641205, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=535, Loss=0.009311305359005928, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=536, Loss=0.021795928478240967, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=537, Loss=0.036764759570360184, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=538, Loss=0.025573154911398888, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=539, Loss=0.017012925818562508, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=540, Loss=0.026985498145222664, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=541, Loss=0.03376714885234833, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=542, Loss=0.029408028349280357, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=543, Loss=0.014693813398480415, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=544, Loss=0.02597847767174244, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=545, Loss=0.02086065709590912, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=546, Loss=0.011990515515208244, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=547, Loss=0.016573378816246986, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=548, Loss=0.049593303352594376, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=549, Loss=0.019068578258156776, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=550, Loss=0.025761494413018227, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=551, Loss=0.017861714586615562, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=552, Loss=0.018155386671423912, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=553, Loss=0.04054625332355499, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=554, Loss=0.022897284477949142, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=555, Loss=0.011216937564313412, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=556, Loss=0.023323802277445793, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=557, Loss=0.040498703718185425, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=558, Loss=0.02637704648077488, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=559, Loss=0.008324680849909782, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=560, Loss=0.020734643563628197, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=561, Loss=0.012696651741862297, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=562, Loss=0.03179750218987465, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=563, Loss=0.016177987679839134, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=564, Loss=0.016179801896214485, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=565, Loss=0.026918960735201836, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=566, Loss=0.021345386281609535, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=567, Loss=0.04184906929731369, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=568, Loss=0.047260627150535583, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=569, Loss=0.016281982883810997, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=570, Loss=0.023424779996275902, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=571, Loss=0.00850347988307476, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=572, Loss=0.026080915704369545, Train=1.0, Val=0.74, Test=0.807\n",
      "Epoch=573, Loss=0.02384667843580246, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=574, Loss=0.013460518792271614, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=575, Loss=0.029177198186516762, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=576, Loss=0.01453512441366911, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=577, Loss=0.021033382043242455, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=578, Loss=0.015036085620522499, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=579, Loss=0.028483256697654724, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=580, Loss=0.018188312649726868, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=581, Loss=0.03602086380124092, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=582, Loss=0.024023395031690598, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=583, Loss=0.03945906460285187, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=584, Loss=0.028667807579040527, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=585, Loss=0.014283312484622002, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=586, Loss=0.021145932376384735, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=587, Loss=0.028736012056469917, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=588, Loss=0.031587257981300354, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=589, Loss=0.03032827004790306, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=590, Loss=0.0316103994846344, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=591, Loss=0.050236426293849945, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=592, Loss=0.01651012897491455, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=593, Loss=0.017493337392807007, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=594, Loss=0.03928220272064209, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=595, Loss=0.027621358633041382, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=596, Loss=0.016369959339499474, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=597, Loss=0.032732587307691574, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=598, Loss=0.021226759999990463, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=599, Loss=0.012958099134266376, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=600, Loss=0.017202962189912796, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=601, Loss=0.023145712912082672, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=602, Loss=0.030677461996674538, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=603, Loss=0.0125456303358078, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=604, Loss=0.028231315314769745, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=605, Loss=0.025784416124224663, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=606, Loss=0.010258471593260765, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=607, Loss=0.016986660659313202, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=608, Loss=0.015159273520112038, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=609, Loss=0.03219415619969368, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=610, Loss=0.04376688227057457, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=611, Loss=0.016605380922555923, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=612, Loss=0.027652624994516373, Train=1.0, Val=0.788, Test=0.804\n",
      "Epoch=613, Loss=0.02162996307015419, Train=1.0, Val=0.786, Test=0.804\n",
      "Epoch=614, Loss=0.01681341417133808, Train=1.0, Val=0.786, Test=0.804\n",
      "Epoch=615, Loss=0.0241912342607975, Train=1.0, Val=0.784, Test=0.804\n",
      "Epoch=616, Loss=0.021966122090816498, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=617, Loss=0.010891921818256378, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=618, Loss=0.034187935292720795, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=619, Loss=0.009369541890919209, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=620, Loss=0.014641529880464077, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=621, Loss=0.012836034409701824, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=622, Loss=0.021739337593317032, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=623, Loss=0.00799411442130804, Train=1.0, Val=0.764, Test=0.804\n",
      "Epoch=624, Loss=0.018083618953824043, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=625, Loss=0.014566571451723576, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=626, Loss=0.009337595663964748, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=627, Loss=0.025614740327000618, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=628, Loss=0.03878837451338768, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=629, Loss=0.027234578505158424, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=630, Loss=0.030482279136776924, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=631, Loss=0.013331868685781956, Train=1.0, Val=0.776, Test=0.804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=632, Loss=0.04037942737340927, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=633, Loss=0.023577652871608734, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=634, Loss=0.014445647597312927, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=635, Loss=0.011791950091719627, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=636, Loss=0.02567153424024582, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=637, Loss=0.02752057835459709, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=638, Loss=0.022650783881545067, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=639, Loss=0.02183237485587597, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=640, Loss=0.017885880544781685, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=641, Loss=0.015092440880835056, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=642, Loss=0.018251091241836548, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=643, Loss=0.012577413581311703, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=644, Loss=0.027091115713119507, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=645, Loss=0.035788435488939285, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=646, Loss=0.022208569571375847, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=647, Loss=0.026455363258719444, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=648, Loss=0.023417025804519653, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=649, Loss=0.020515218377113342, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=650, Loss=0.022952593863010406, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=651, Loss=0.014038565568625927, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=652, Loss=0.026208382099866867, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=653, Loss=0.033776428550481796, Train=1.0, Val=0.764, Test=0.804\n",
      "Epoch=654, Loss=0.025311123579740524, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=655, Loss=0.009829536080360413, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=656, Loss=0.03581961989402771, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=657, Loss=0.0360289141535759, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=658, Loss=0.02302374318242073, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=659, Loss=0.022332530468702316, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=660, Loss=0.012057761661708355, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=661, Loss=0.03324401378631592, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=662, Loss=0.06445051729679108, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=663, Loss=0.00819360464811325, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=664, Loss=0.02032540738582611, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=665, Loss=0.01834068074822426, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=666, Loss=0.01920248568058014, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=667, Loss=0.009080259129405022, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=668, Loss=0.017920177429914474, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=669, Loss=0.018468093127012253, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=670, Loss=0.02191849984228611, Train=1.0, Val=0.764, Test=0.804\n",
      "Epoch=671, Loss=0.014934996142983437, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=672, Loss=0.04445662721991539, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=673, Loss=0.026825670152902603, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=674, Loss=0.013810370117425919, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=675, Loss=0.052273478358983994, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=676, Loss=0.010304078459739685, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=677, Loss=0.009393607266247272, Train=1.0, Val=0.75, Test=0.804\n",
      "Epoch=678, Loss=0.022708773612976074, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=679, Loss=0.01819007471203804, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=680, Loss=0.011738436296582222, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=681, Loss=0.018425228074193, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=682, Loss=0.01798694021999836, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=683, Loss=0.02218802645802498, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=684, Loss=0.021082237362861633, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=685, Loss=0.023925291374325752, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=686, Loss=0.015397791750729084, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=687, Loss=0.022608544677495956, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=688, Loss=0.03820062428712845, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=689, Loss=0.011258398182690144, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=690, Loss=0.02625596709549427, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=691, Loss=0.01167997159063816, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=692, Loss=0.0107271708548069, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=693, Loss=0.02251657284796238, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=694, Loss=0.02027372643351555, Train=1.0, Val=0.764, Test=0.804\n",
      "Epoch=695, Loss=0.007781770545989275, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=696, Loss=0.020604567602276802, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=697, Loss=0.0155648710206151, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=698, Loss=0.04269883409142494, Train=1.0, Val=0.748, Test=0.804\n",
      "Epoch=699, Loss=0.026895718649029732, Train=1.0, Val=0.75, Test=0.804\n",
      "Epoch=700, Loss=0.01607191003859043, Train=1.0, Val=0.746, Test=0.804\n",
      "Epoch=701, Loss=0.0133446604013443, Train=1.0, Val=0.742, Test=0.804\n",
      "Epoch=702, Loss=0.021170925348997116, Train=1.0, Val=0.746, Test=0.804\n",
      "Epoch=703, Loss=0.015595964156091213, Train=1.0, Val=0.742, Test=0.804\n",
      "Epoch=704, Loss=0.04539448022842407, Train=1.0, Val=0.742, Test=0.804\n",
      "Epoch=705, Loss=0.016188809648156166, Train=1.0, Val=0.748, Test=0.804\n",
      "Epoch=706, Loss=0.022055020555853844, Train=1.0, Val=0.746, Test=0.804\n",
      "Epoch=707, Loss=0.013304241001605988, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=708, Loss=0.016486387699842453, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=709, Loss=0.031115815043449402, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=710, Loss=0.03350655362010002, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=711, Loss=0.044435106217861176, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=712, Loss=0.014933318831026554, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=713, Loss=0.016989609226584435, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=714, Loss=0.012410241179168224, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=715, Loss=0.028490161523222923, Train=1.0, Val=0.77, Test=0.804\n",
      "Epoch=716, Loss=0.015471608377993107, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=717, Loss=0.011441203765571117, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=718, Loss=0.02143019251525402, Train=1.0, Val=0.766, Test=0.804\n",
      "Epoch=719, Loss=0.02232036180794239, Train=1.0, Val=0.764, Test=0.804\n",
      "Epoch=720, Loss=0.04852055758237839, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=721, Loss=0.014003485441207886, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=722, Loss=0.02348519116640091, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=723, Loss=0.06673119962215424, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=724, Loss=0.016542790457606316, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=725, Loss=0.028633680194616318, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=726, Loss=0.04402996227145195, Train=1.0, Val=0.76, Test=0.804\n",
      "Epoch=727, Loss=0.027303874492645264, Train=1.0, Val=0.754, Test=0.804\n",
      "Epoch=728, Loss=0.029609203338623047, Train=1.0, Val=0.752, Test=0.804\n",
      "Epoch=729, Loss=0.016231607645750046, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=730, Loss=0.020720573142170906, Train=1.0, Val=0.758, Test=0.804\n",
      "Epoch=731, Loss=0.016159577295184135, Train=1.0, Val=0.756, Test=0.804\n",
      "Epoch=732, Loss=0.022830167785286903, Train=1.0, Val=0.762, Test=0.804\n",
      "Epoch=733, Loss=0.02432982251048088, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=734, Loss=0.015270761214196682, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=735, Loss=0.02493050880730152, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=736, Loss=0.029582319781184196, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=737, Loss=0.03926580399274826, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=738, Loss=0.01614992693066597, Train=1.0, Val=0.784, Test=0.804\n",
      "Epoch=739, Loss=0.012853214517235756, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=740, Loss=0.023371336981654167, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=741, Loss=0.025787435472011566, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=742, Loss=0.019482942298054695, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=743, Loss=0.019768068566918373, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=744, Loss=0.018929317593574524, Train=1.0, Val=0.772, Test=0.804\n",
      "Epoch=745, Loss=0.0105054946616292, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=746, Loss=0.030027961358428, Train=1.0, Val=0.768, Test=0.804\n",
      "Epoch=747, Loss=0.024883603677153587, Train=1.0, Val=0.774, Test=0.804\n",
      "Epoch=748, Loss=0.010192528367042542, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=749, Loss=0.04318321868777275, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=750, Loss=0.0273666400462389, Train=1.0, Val=0.78, Test=0.804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=751, Loss=0.04612617567181587, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=752, Loss=0.016979079693555832, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=753, Loss=0.010357926599681377, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=754, Loss=0.04292763024568558, Train=1.0, Val=0.776, Test=0.804\n",
      "Epoch=755, Loss=0.047181855887174606, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=756, Loss=0.018498258665204048, Train=1.0, Val=0.788, Test=0.804\n",
      "Epoch=757, Loss=0.022412627935409546, Train=1.0, Val=0.788, Test=0.804\n",
      "Epoch=758, Loss=0.016340719535946846, Train=1.0, Val=0.782, Test=0.804\n",
      "Epoch=759, Loss=0.02195344679057598, Train=1.0, Val=0.778, Test=0.804\n",
      "Epoch=760, Loss=0.02002965845167637, Train=1.0, Val=0.78, Test=0.804\n",
      "Epoch=761, Loss=0.015160567127168179, Train=1.0, Val=0.788, Test=0.804\n",
      "Epoch=762, Loss=0.013879523612558842, Train=1.0, Val=0.788, Test=0.804\n",
      "Epoch=763, Loss=0.026030313223600388, Train=1.0, Val=0.79, Test=0.797\n",
      "Epoch=764, Loss=0.0201549232006073, Train=1.0, Val=0.788, Test=0.797\n",
      "Epoch=765, Loss=0.014618100598454475, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=766, Loss=0.016550537198781967, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=767, Loss=0.044697266072034836, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=768, Loss=0.011192853562533855, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=769, Loss=0.02106870338320732, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=770, Loss=0.021519146859645844, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=771, Loss=0.03231383115053177, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=772, Loss=0.03233838826417923, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=773, Loss=0.026105066761374474, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=774, Loss=0.016031332314014435, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=775, Loss=0.045497000217437744, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=776, Loss=0.01706492342054844, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=777, Loss=0.023246537894010544, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=778, Loss=0.02443745546042919, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=779, Loss=0.009896189905703068, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=780, Loss=0.038875676691532135, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=781, Loss=0.008587749674916267, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=782, Loss=0.0322272889316082, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=783, Loss=0.04467504844069481, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=784, Loss=0.012260526418685913, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=785, Loss=0.06033989414572716, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=786, Loss=0.047289084643125534, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=787, Loss=0.03929267078638077, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=788, Loss=0.033942703157663345, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=789, Loss=0.018579399213194847, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=790, Loss=0.05120189115405083, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=791, Loss=0.030864572152495384, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=792, Loss=0.018364261835813522, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=793, Loss=0.028593100607395172, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=794, Loss=0.022864436730742455, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=795, Loss=0.015298177488148212, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=796, Loss=0.01734074391424656, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=797, Loss=0.01976092904806137, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=798, Loss=0.019366830587387085, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=799, Loss=0.01887514442205429, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=800, Loss=0.01834275759756565, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=801, Loss=0.014679057523608208, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=802, Loss=0.03555675595998764, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=803, Loss=0.01875166967511177, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=804, Loss=0.013051343150436878, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=805, Loss=0.017155252397060394, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=806, Loss=0.026519332081079483, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=807, Loss=0.007884048856794834, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=808, Loss=0.034146491438150406, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=809, Loss=0.010038298554718494, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=810, Loss=0.015221696346998215, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=811, Loss=0.027988053858280182, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=812, Loss=0.010235327295958996, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=813, Loss=0.039814721792936325, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=814, Loss=0.020388392731547356, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=815, Loss=0.0322832353413105, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=816, Loss=0.009342746809124947, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=817, Loss=0.011121409013867378, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=818, Loss=0.009961501695215702, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=819, Loss=0.022899635136127472, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=820, Loss=0.016124188899993896, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=821, Loss=0.03911556303501129, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=822, Loss=0.024061281234025955, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=823, Loss=0.011117811314761639, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=824, Loss=0.01338900625705719, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=825, Loss=0.015568125061690807, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=826, Loss=0.004118246491998434, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=827, Loss=0.01399727538228035, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=828, Loss=0.010416707955300808, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=829, Loss=0.0367945060133934, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=830, Loss=0.01728472299873829, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=831, Loss=0.038337592035532, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=832, Loss=0.03753652051091194, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=833, Loss=0.02242036908864975, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=834, Loss=0.02303391322493553, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=835, Loss=0.03802802786231041, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=836, Loss=0.016545284539461136, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=837, Loss=0.008877876214683056, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=838, Loss=0.022211268544197083, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=839, Loss=0.03049013577401638, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=840, Loss=0.030846895650029182, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=841, Loss=0.016367048025131226, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=842, Loss=0.013036961667239666, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=843, Loss=0.031496547162532806, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=844, Loss=0.012539033778011799, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=845, Loss=0.020413506776094437, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=846, Loss=0.01816203072667122, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=847, Loss=0.019197089597582817, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=848, Loss=0.02308281697332859, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=849, Loss=0.041106004267930984, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=850, Loss=0.021374545991420746, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=851, Loss=0.024061689153313637, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=852, Loss=0.015460913069546223, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=853, Loss=0.020135676488280296, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=854, Loss=0.023005817085504532, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=855, Loss=0.015325362794101238, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=856, Loss=0.014882556162774563, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=857, Loss=0.01790071278810501, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=858, Loss=0.016099629923701286, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=859, Loss=0.030490845441818237, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=860, Loss=0.016140632331371307, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=861, Loss=0.024639196693897247, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=862, Loss=0.021323710680007935, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=863, Loss=0.028459874913096428, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=864, Loss=0.02537020668387413, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=865, Loss=0.026895126327872276, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=866, Loss=0.015570975840091705, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=867, Loss=0.029836580157279968, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=868, Loss=0.013946856372058392, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=869, Loss=0.006116929464042187, Train=1.0, Val=0.768, Test=0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=870, Loss=0.04812312498688698, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=871, Loss=0.013640360906720161, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=872, Loss=0.01842815801501274, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=873, Loss=0.017656566575169563, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=874, Loss=0.028088601306080818, Train=1.0, Val=0.79, Test=0.797\n",
      "Epoch=875, Loss=0.009358661249279976, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=876, Loss=0.006680448539555073, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=877, Loss=0.02278836816549301, Train=1.0, Val=0.788, Test=0.797\n",
      "Epoch=878, Loss=0.015987789258360863, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=879, Loss=0.017684804275631905, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=880, Loss=0.017501775175333023, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=881, Loss=0.02343592792749405, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=882, Loss=0.016368065029382706, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=883, Loss=0.019733628258109093, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=884, Loss=0.010736781172454357, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=885, Loss=0.016032574698328972, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=886, Loss=0.01556918304413557, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=887, Loss=0.026249241083860397, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=888, Loss=0.010656310245394707, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=889, Loss=0.017740633338689804, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=890, Loss=0.020735612139105797, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=891, Loss=0.05724973604083061, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=892, Loss=0.01299914252012968, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=893, Loss=0.018969018012285233, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=894, Loss=0.023988330736756325, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=895, Loss=0.007018434815108776, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=896, Loss=0.028245260939002037, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=897, Loss=0.013585032895207405, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=898, Loss=0.019856445491313934, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=899, Loss=0.013956570066511631, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=900, Loss=0.007638184353709221, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=901, Loss=0.01881404221057892, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=902, Loss=0.03349119424819946, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=903, Loss=0.03830534964799881, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=904, Loss=0.018289480358362198, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=905, Loss=0.03470117226243019, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=906, Loss=0.016652541235089302, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=907, Loss=0.013796494342386723, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=908, Loss=0.015467592515051365, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=909, Loss=0.01859450340270996, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=910, Loss=0.019823674112558365, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=911, Loss=0.02253265306353569, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=912, Loss=0.01024651899933815, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=913, Loss=0.035306982696056366, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=914, Loss=0.026331067085266113, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=915, Loss=0.022368336096405983, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=916, Loss=0.013719189912080765, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=917, Loss=0.011639099568128586, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=918, Loss=0.007692326791584492, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=919, Loss=0.01833856664597988, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=920, Loss=0.01660844124853611, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=921, Loss=0.021976230666041374, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=922, Loss=0.01350060012191534, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=923, Loss=0.013744872063398361, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=924, Loss=0.022004973143339157, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=925, Loss=0.024354703724384308, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=926, Loss=0.022838560864329338, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=927, Loss=0.02151920273900032, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=928, Loss=0.012387178838253021, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=929, Loss=0.026954153552651405, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=930, Loss=0.00970750767737627, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=931, Loss=0.009294865652918816, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=932, Loss=0.01300680823624134, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=933, Loss=0.006965456530451775, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=934, Loss=0.01961599290370941, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=935, Loss=0.011441917158663273, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=936, Loss=0.006859412882477045, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=937, Loss=0.02996954880654812, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=938, Loss=0.016858788207173347, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=939, Loss=0.008953392505645752, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=940, Loss=0.02043147012591362, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=941, Loss=0.01585152931511402, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=942, Loss=0.005682011134922504, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=943, Loss=0.015106484293937683, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=944, Loss=0.019887002184987068, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=945, Loss=0.026642026379704475, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=946, Loss=0.013458375819027424, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=947, Loss=0.013343172147870064, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=948, Loss=0.009345545433461666, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=949, Loss=0.02312525175511837, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=950, Loss=0.02492632530629635, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=951, Loss=0.02264743112027645, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=952, Loss=0.019270339980721474, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=953, Loss=0.011622061021625996, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=954, Loss=0.009648045524954796, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=955, Loss=0.014720828272402287, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=956, Loss=0.012526081874966621, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=957, Loss=0.03275413066148758, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=958, Loss=0.02571975253522396, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=959, Loss=0.019845161586999893, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=960, Loss=0.011439736001193523, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=961, Loss=0.02480979450047016, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=962, Loss=0.02887864038348198, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=963, Loss=0.0233660526573658, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=964, Loss=0.013615714386105537, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=965, Loss=0.03001134656369686, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=966, Loss=0.021382102742791176, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=967, Loss=0.012218649499118328, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=968, Loss=0.0139648811891675, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=969, Loss=0.02959580346941948, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=970, Loss=0.023747961968183517, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=971, Loss=0.01111895963549614, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=972, Loss=0.023744981735944748, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=973, Loss=0.006348088849335909, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=974, Loss=0.03755176439881325, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=975, Loss=0.018210778012871742, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=976, Loss=0.010401489213109016, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=977, Loss=0.04608667269349098, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=978, Loss=0.03008568286895752, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=979, Loss=0.019592395052313805, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=980, Loss=0.033301085233688354, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=981, Loss=0.012478338554501534, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=982, Loss=0.03366934135556221, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=983, Loss=0.027156032621860504, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=984, Loss=0.019233938306570053, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=985, Loss=0.016922781243920326, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=986, Loss=0.03888900205492973, Train=1.0, Val=0.754, Test=0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=987, Loss=0.01085219532251358, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=988, Loss=0.006185899954289198, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=989, Loss=0.020215284079313278, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=990, Loss=0.010591946542263031, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=991, Loss=0.019004762172698975, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=992, Loss=0.03518274053931236, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=993, Loss=0.028691932559013367, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=994, Loss=0.03437649831175804, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=995, Loss=0.011362050659954548, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=996, Loss=0.05423074960708618, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=997, Loss=0.011267537251114845, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=998, Loss=0.03515677526593208, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=999, Loss=0.020858794450759888, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1000, Loss=0.03079369105398655, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1001, Loss=0.0483393594622612, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1002, Loss=0.016735920682549477, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1003, Loss=0.022396773099899292, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1004, Loss=0.020311979576945305, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1005, Loss=0.012418961152434349, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1006, Loss=0.020300498232245445, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1007, Loss=0.03059878945350647, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1008, Loss=0.036482471972703934, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1009, Loss=0.015357228927314281, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1010, Loss=0.006101530976593494, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1011, Loss=0.02678292989730835, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1012, Loss=0.03311150148510933, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1013, Loss=0.022047335281968117, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1014, Loss=0.016801362857222557, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1015, Loss=0.022872809320688248, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1016, Loss=0.01940605603158474, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1017, Loss=0.01995699666440487, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1018, Loss=0.021095797419548035, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1019, Loss=0.012223795987665653, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1020, Loss=0.026860367506742477, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1021, Loss=0.01608869433403015, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1022, Loss=0.02149844728410244, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1023, Loss=0.011888000182807446, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1024, Loss=0.010092321783304214, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1025, Loss=0.012181879952549934, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1026, Loss=0.01186980027705431, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1027, Loss=0.008617219515144825, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1028, Loss=0.006615343037992716, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1029, Loss=0.021091390401124954, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1030, Loss=0.02145622856914997, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1031, Loss=0.026199787855148315, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1032, Loss=0.00455721328034997, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1033, Loss=0.01761198416352272, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1034, Loss=0.024662643671035767, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1035, Loss=0.02927154116332531, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1036, Loss=0.03576013073325157, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1037, Loss=0.05087366700172424, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1038, Loss=0.027709053829312325, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1039, Loss=0.010411154478788376, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1040, Loss=0.027465717867016792, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1041, Loss=0.01083091925829649, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1042, Loss=0.03161622956395149, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1043, Loss=0.01235725823789835, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1044, Loss=0.025809237733483315, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1045, Loss=0.03795016556978226, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1046, Loss=0.014612870290875435, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1047, Loss=0.01803777739405632, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1048, Loss=0.009503777138888836, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1049, Loss=0.015325069427490234, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1050, Loss=0.008417272008955479, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1051, Loss=0.016523653641343117, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1052, Loss=0.03227394074201584, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1053, Loss=0.021217869594693184, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1054, Loss=0.02019883133471012, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1055, Loss=0.023548752069473267, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1056, Loss=0.013471012003719807, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1057, Loss=0.020677126944065094, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1058, Loss=0.018444610759615898, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1059, Loss=0.016797656193375587, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1060, Loss=0.017313649877905846, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1061, Loss=0.017965862527489662, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1062, Loss=0.013638337142765522, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1063, Loss=0.02509322203695774, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1064, Loss=0.011780394241213799, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1065, Loss=0.03288714215159416, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1066, Loss=0.01607409305870533, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1067, Loss=0.014675414189696312, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1068, Loss=0.025919698178768158, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1069, Loss=0.00931554101407528, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1070, Loss=0.01366790384054184, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1071, Loss=0.010203845798969269, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1072, Loss=0.023245908319950104, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1073, Loss=0.0148012675344944, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1074, Loss=0.031512197107076645, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1075, Loss=0.01731087453663349, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1076, Loss=0.028199326246976852, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1077, Loss=0.014785906299948692, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1078, Loss=0.017322465777397156, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1079, Loss=0.02145766280591488, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1080, Loss=0.014596785418689251, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1081, Loss=0.013852044939994812, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1082, Loss=0.060040201991796494, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1083, Loss=0.036972109228372574, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1084, Loss=0.016372082754969597, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1085, Loss=0.012679863721132278, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1086, Loss=0.019038431346416473, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1087, Loss=0.014847652986645699, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1088, Loss=0.013034536503255367, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1089, Loss=0.011851399205625057, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1090, Loss=0.021864721551537514, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1091, Loss=0.035353757441043854, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1092, Loss=0.00997941568493843, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1093, Loss=0.015611613169312477, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1094, Loss=0.022621337324380875, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1095, Loss=0.02255706489086151, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1096, Loss=0.034721799194812775, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1097, Loss=0.028413323685526848, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1098, Loss=0.006315434351563454, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1099, Loss=0.03485138714313507, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1100, Loss=0.015995735302567482, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1101, Loss=0.009597293101251125, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1102, Loss=0.022545667365193367, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1103, Loss=0.03601783514022827, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1104, Loss=0.020723000168800354, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1105, Loss=0.016018612310290337, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1106, Loss=0.015515368431806564, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1107, Loss=0.027273092418909073, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1108, Loss=0.009766665287315845, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1109, Loss=0.016338570043444633, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1110, Loss=0.03175779804587364, Train=1.0, Val=0.766, Test=0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1111, Loss=0.015785740688443184, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1112, Loss=0.018901066854596138, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1113, Loss=0.03795702010393143, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1114, Loss=0.01673583872616291, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1115, Loss=0.0256801825016737, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1116, Loss=0.02038249745965004, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1117, Loss=0.018547168001532555, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1118, Loss=0.01862100511789322, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1119, Loss=0.01125680934637785, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1120, Loss=0.018675649538636208, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1121, Loss=0.014841611497104168, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1122, Loss=0.01103291753679514, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1123, Loss=0.02399957738816738, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1124, Loss=0.014805074781179428, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1125, Loss=0.024068238213658333, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1126, Loss=0.013163321651518345, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1127, Loss=0.019946962594985962, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1128, Loss=0.011123119853436947, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1129, Loss=0.016149140894412994, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1130, Loss=0.02282118797302246, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1131, Loss=0.007117422763258219, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1132, Loss=0.02503204718232155, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1133, Loss=0.02180478163063526, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1134, Loss=0.027206966653466225, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1135, Loss=0.005036657676100731, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1136, Loss=0.015765387564897537, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1137, Loss=0.026817768812179565, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1138, Loss=0.02687608264386654, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1139, Loss=0.012804712168872356, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1140, Loss=0.00937865674495697, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1141, Loss=0.007533373311161995, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1142, Loss=0.03618660196661949, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1143, Loss=0.01648586615920067, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1144, Loss=0.019420256838202477, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1145, Loss=0.019258668646216393, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1146, Loss=0.0187994297593832, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1147, Loss=0.01962444745004177, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1148, Loss=0.035254478454589844, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1149, Loss=0.018389781937003136, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1150, Loss=0.013275218196213245, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1151, Loss=0.01099091861397028, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1152, Loss=0.02401077188551426, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1153, Loss=0.01154150627553463, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1154, Loss=0.017165660858154297, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1155, Loss=0.016915934160351753, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1156, Loss=0.008632278069853783, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1157, Loss=0.01645471341907978, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1158, Loss=0.021859977394342422, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1159, Loss=0.019951054826378822, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1160, Loss=0.012126786634325981, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1161, Loss=0.022585174068808556, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1162, Loss=0.007734139449894428, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1163, Loss=0.021451573818922043, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1164, Loss=0.017512716352939606, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1165, Loss=0.0183172095566988, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1166, Loss=0.010698982514441013, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1167, Loss=0.01328057050704956, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1168, Loss=0.032791271805763245, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1169, Loss=0.009848708286881447, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1170, Loss=0.011806528083980083, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1171, Loss=0.018168380483984947, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1172, Loss=0.01173842791467905, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1173, Loss=0.011771426536142826, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1174, Loss=0.018277756869792938, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1175, Loss=0.028264611959457397, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1176, Loss=0.013052756898105145, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1177, Loss=0.032794732600450516, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1178, Loss=0.010305034928023815, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1179, Loss=0.0156342051923275, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1180, Loss=0.011211443692445755, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1181, Loss=0.020322510972619057, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1182, Loss=0.018664101138710976, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1183, Loss=0.029650548473000526, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1184, Loss=0.04971983656287193, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1185, Loss=0.008668840862810612, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1186, Loss=0.030478201806545258, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1187, Loss=0.01588415540754795, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1188, Loss=0.005902560893446207, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1189, Loss=0.035276081413030624, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1190, Loss=0.020802602171897888, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1191, Loss=0.02457132376730442, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1192, Loss=0.0055806925520300865, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1193, Loss=0.014327610842883587, Train=1.0, Val=0.746, Test=0.797\n",
      "Epoch=1194, Loss=0.01600061170756817, Train=1.0, Val=0.744, Test=0.797\n",
      "Epoch=1195, Loss=0.0074773081578314304, Train=1.0, Val=0.742, Test=0.797\n",
      "Epoch=1196, Loss=0.006869176868349314, Train=1.0, Val=0.74, Test=0.797\n",
      "Epoch=1197, Loss=0.029518723487854004, Train=1.0, Val=0.738, Test=0.797\n",
      "Epoch=1198, Loss=0.008260629139840603, Train=1.0, Val=0.734, Test=0.797\n",
      "Epoch=1199, Loss=0.021412745118141174, Train=1.0, Val=0.74, Test=0.797\n",
      "Epoch=1200, Loss=0.015432308427989483, Train=1.0, Val=0.744, Test=0.797\n",
      "Epoch=1201, Loss=0.031211918219923973, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1202, Loss=0.010698976926505566, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1203, Loss=0.025294817984104156, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1204, Loss=0.01000442635267973, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1205, Loss=0.037232656031847, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1206, Loss=0.009948980994522572, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1207, Loss=0.009967964142560959, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1208, Loss=0.012417818419635296, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1209, Loss=0.016328781843185425, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1210, Loss=0.01144859567284584, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1211, Loss=0.03058604523539543, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1212, Loss=0.007299524731934071, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1213, Loss=0.008800056762993336, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1214, Loss=0.015577190555632114, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1215, Loss=0.019230300560593605, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1216, Loss=0.01119930762797594, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1217, Loss=0.016618074849247932, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1218, Loss=0.015233506448566914, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1219, Loss=0.015806786715984344, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1220, Loss=0.01894833892583847, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1221, Loss=0.022832587361335754, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1222, Loss=0.010231385007500648, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1223, Loss=0.013486132025718689, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1224, Loss=0.02826165221631527, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1225, Loss=0.0174466073513031, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1226, Loss=0.023546801880002022, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1227, Loss=0.010085491463541985, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1228, Loss=0.01905050128698349, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1229, Loss=0.017268836498260498, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1230, Loss=0.014508144930005074, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1231, Loss=0.023488767445087433, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1232, Loss=0.008555946871638298, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1233, Loss=0.015671415254473686, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1234, Loss=0.013199245557188988, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1235, Loss=0.003609264735132456, Train=1.0, Val=0.77, Test=0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1236, Loss=0.02185213565826416, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1237, Loss=0.013614602386951447, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1238, Loss=0.021478090435266495, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1239, Loss=0.005558427423238754, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1240, Loss=0.0259830504655838, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1241, Loss=0.028758730739355087, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1242, Loss=0.014464691281318665, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1243, Loss=0.0209768395870924, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1244, Loss=0.02273806743323803, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1245, Loss=0.0219670832157135, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1246, Loss=0.026848746463656425, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1247, Loss=0.02032012678682804, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1248, Loss=0.013588936068117619, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1249, Loss=0.006509698461741209, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1250, Loss=0.023840568959712982, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1251, Loss=0.023000765591859818, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1252, Loss=0.034740835428237915, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1253, Loss=0.010951140895485878, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1254, Loss=0.012811824679374695, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1255, Loss=0.017331378534436226, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1256, Loss=0.016021739691495895, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1257, Loss=0.012611950747668743, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1258, Loss=0.01228358969092369, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1259, Loss=0.04774978756904602, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1260, Loss=0.01422708760946989, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1261, Loss=0.02185276336967945, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1262, Loss=0.02095498889684677, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1263, Loss=0.020034288987517357, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1264, Loss=0.0349733941257, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1265, Loss=0.019713232293725014, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1266, Loss=0.019955387338995934, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1267, Loss=0.01944875344634056, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1268, Loss=0.015546665526926517, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1269, Loss=0.01741076447069645, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1270, Loss=0.02775798738002777, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1271, Loss=0.01626746915280819, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1272, Loss=0.009848949499428272, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1273, Loss=0.025534039363265038, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1274, Loss=0.04287497326731682, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1275, Loss=0.03617780655622482, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1276, Loss=0.014257828705012798, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1277, Loss=0.03893852233886719, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1278, Loss=0.010867789387702942, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1279, Loss=0.01343446597456932, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1280, Loss=0.034352920949459076, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1281, Loss=0.019207069650292397, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1282, Loss=0.011920436285436153, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1283, Loss=0.011915363371372223, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1284, Loss=0.03341112285852432, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1285, Loss=0.018264539539813995, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1286, Loss=0.00963855255395174, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1287, Loss=0.0063325162045657635, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1288, Loss=0.011654479429125786, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1289, Loss=0.017906343564391136, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1290, Loss=0.027428869158029556, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1291, Loss=0.03225918114185333, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1292, Loss=0.011696738190948963, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1293, Loss=0.00985035952180624, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1294, Loss=0.005385271739214659, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1295, Loss=0.0253366157412529, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1296, Loss=0.02511301450431347, Train=1.0, Val=0.776, Test=0.797\n",
      "Epoch=1297, Loss=0.012985044158995152, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1298, Loss=0.010320674628019333, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1299, Loss=0.029488300904631615, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=1300, Loss=0.018053436651825905, Train=1.0, Val=0.79, Test=0.797\n",
      "Epoch=1301, Loss=0.018764164298772812, Train=1.0, Val=0.788, Test=0.797\n",
      "Epoch=1302, Loss=0.010900434106588364, Train=1.0, Val=0.788, Test=0.797\n",
      "Epoch=1303, Loss=0.03424079343676567, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=1304, Loss=0.01697542332112789, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=1305, Loss=0.030859721824526787, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1306, Loss=0.008574859239161015, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1307, Loss=0.01446090079843998, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1308, Loss=0.018931236118078232, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1309, Loss=0.02040969207882881, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1310, Loss=0.012460917234420776, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1311, Loss=0.01562597043812275, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1312, Loss=0.009204997681081295, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1313, Loss=0.020684832707047462, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1314, Loss=0.017561357468366623, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1315, Loss=0.005511782597750425, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1316, Loss=0.009595150128006935, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1317, Loss=0.031677450984716415, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1318, Loss=0.01533442735671997, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1319, Loss=0.008153597824275494, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1320, Loss=0.02408416010439396, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1321, Loss=0.028223445639014244, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1322, Loss=0.02653796784579754, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1323, Loss=0.006552118808031082, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1324, Loss=0.02128126658499241, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1325, Loss=0.0434873104095459, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1326, Loss=0.016939455643296242, Train=1.0, Val=0.758, Test=0.797\n",
      "Epoch=1327, Loss=0.017391396686434746, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1328, Loss=0.02217043936252594, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1329, Loss=0.024216465651988983, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1330, Loss=0.007637623231858015, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1331, Loss=0.013337211683392525, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1332, Loss=0.024381089955568314, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1333, Loss=0.037954431027173996, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1334, Loss=0.030237330123782158, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1335, Loss=0.012739686295390129, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1336, Loss=0.011684193275868893, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1337, Loss=0.01348119042813778, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1338, Loss=0.028932642191648483, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1339, Loss=0.011431480757892132, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1340, Loss=0.04239882901310921, Train=1.0, Val=0.764, Test=0.797\n",
      "Epoch=1341, Loss=0.01890367828309536, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1342, Loss=0.022033734247088432, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1343, Loss=0.022582048550248146, Train=1.0, Val=0.768, Test=0.797\n",
      "Epoch=1344, Loss=0.015594248659908772, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1345, Loss=0.02620072104036808, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1346, Loss=0.009930051863193512, Train=1.0, Val=0.75, Test=0.797\n",
      "Epoch=1347, Loss=0.031269341707229614, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1348, Loss=0.020713260397315025, Train=1.0, Val=0.752, Test=0.797\n",
      "Epoch=1349, Loss=0.019707491621375084, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1350, Loss=0.008228376507759094, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1351, Loss=0.0224078930914402, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1352, Loss=0.02442687749862671, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1353, Loss=0.018726404756307602, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1354, Loss=0.010422782972455025, Train=1.0, Val=0.76, Test=0.797\n",
      "Epoch=1355, Loss=0.02175460197031498, Train=1.0, Val=0.762, Test=0.797\n",
      "Epoch=1356, Loss=0.02841947227716446, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1357, Loss=0.008950229734182358, Train=1.0, Val=0.768, Test=0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1358, Loss=0.03614841029047966, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1359, Loss=0.024986734613776207, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1360, Loss=0.012663505971431732, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1361, Loss=0.011645880527794361, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1362, Loss=0.01664435677230358, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1363, Loss=0.008115922100841999, Train=1.0, Val=0.778, Test=0.797\n",
      "Epoch=1364, Loss=0.03669465333223343, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1365, Loss=0.009089496918022633, Train=1.0, Val=0.77, Test=0.797\n",
      "Epoch=1366, Loss=0.02689094841480255, Train=1.0, Val=0.772, Test=0.797\n",
      "Epoch=1367, Loss=0.009709284640848637, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1368, Loss=0.020197508856654167, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1369, Loss=0.011642874218523502, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1370, Loss=0.020238662138581276, Train=1.0, Val=0.774, Test=0.797\n",
      "Epoch=1371, Loss=0.013149382546544075, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1372, Loss=0.0100157605484128, Train=1.0, Val=0.756, Test=0.797\n",
      "Epoch=1373, Loss=0.010962947271764278, Train=1.0, Val=0.746, Test=0.797\n",
      "Epoch=1374, Loss=0.020816592499613762, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1375, Loss=0.021799709647893906, Train=1.0, Val=0.748, Test=0.797\n",
      "Epoch=1376, Loss=0.04900024086236954, Train=1.0, Val=0.754, Test=0.797\n",
      "Epoch=1377, Loss=0.025208255276083946, Train=1.0, Val=0.766, Test=0.797\n",
      "Epoch=1378, Loss=0.012439325451850891, Train=1.0, Val=0.782, Test=0.797\n",
      "Epoch=1379, Loss=0.012483611702919006, Train=1.0, Val=0.78, Test=0.797\n",
      "Epoch=1380, Loss=0.013419080525636673, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=1381, Loss=0.013551752083003521, Train=1.0, Val=0.784, Test=0.797\n",
      "Epoch=1382, Loss=0.021528981626033783, Train=1.0, Val=0.786, Test=0.797\n",
      "Epoch=1383, Loss=0.0396050401031971, Train=1.0, Val=0.79, Test=0.797\n",
      "Epoch=1384, Loss=0.016424333676695824, Train=1.0, Val=0.792, Test=0.803\n",
      "Epoch=1385, Loss=0.008347098715603352, Train=1.0, Val=0.786, Test=0.803\n",
      "Epoch=1386, Loss=0.03302446007728577, Train=1.0, Val=0.778, Test=0.803\n",
      "Epoch=1387, Loss=0.008507216349244118, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1388, Loss=0.009078622795641422, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1389, Loss=0.012446027249097824, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1390, Loss=0.011730676516890526, Train=1.0, Val=0.77, Test=0.803\n",
      "Epoch=1391, Loss=0.016377177089452744, Train=1.0, Val=0.764, Test=0.803\n",
      "Epoch=1392, Loss=0.0268806591629982, Train=1.0, Val=0.768, Test=0.803\n",
      "Epoch=1393, Loss=0.014582553878426552, Train=1.0, Val=0.774, Test=0.803\n",
      "Epoch=1394, Loss=0.01231376826763153, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=1395, Loss=0.010437045246362686, Train=1.0, Val=0.774, Test=0.803\n",
      "Epoch=1396, Loss=0.009482868015766144, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1397, Loss=0.0310372244566679, Train=1.0, Val=0.782, Test=0.803\n",
      "Epoch=1398, Loss=0.009045389480888844, Train=1.0, Val=0.782, Test=0.803\n",
      "Epoch=1399, Loss=0.01378992386162281, Train=1.0, Val=0.782, Test=0.803\n",
      "Epoch=1400, Loss=0.011637579649686813, Train=1.0, Val=0.786, Test=0.803\n",
      "Epoch=1401, Loss=0.007637085393071175, Train=1.0, Val=0.786, Test=0.803\n",
      "Epoch=1402, Loss=0.008551954291760921, Train=1.0, Val=0.786, Test=0.803\n",
      "Epoch=1403, Loss=0.015249856747686863, Train=1.0, Val=0.788, Test=0.803\n",
      "Epoch=1404, Loss=0.018060771748423576, Train=1.0, Val=0.782, Test=0.803\n",
      "Epoch=1405, Loss=0.029998397454619408, Train=1.0, Val=0.78, Test=0.803\n",
      "Epoch=1406, Loss=0.01420578919351101, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1407, Loss=0.01098597515374422, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=1408, Loss=0.030002301558852196, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=1409, Loss=0.030762318521738052, Train=1.0, Val=0.77, Test=0.803\n",
      "Epoch=1410, Loss=0.02073763869702816, Train=1.0, Val=0.768, Test=0.803\n",
      "Epoch=1411, Loss=0.010105634108185768, Train=1.0, Val=0.768, Test=0.803\n",
      "Epoch=1412, Loss=0.019450755789875984, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=1413, Loss=0.013306553475558758, Train=1.0, Val=0.78, Test=0.803\n",
      "Epoch=1414, Loss=0.0353059321641922, Train=1.0, Val=0.784, Test=0.803\n",
      "Epoch=1415, Loss=0.007478408515453339, Train=1.0, Val=0.782, Test=0.803\n",
      "Epoch=1416, Loss=0.015033270232379436, Train=1.0, Val=0.784, Test=0.803\n",
      "Epoch=1417, Loss=0.011579225771129131, Train=1.0, Val=0.778, Test=0.803\n",
      "Epoch=1418, Loss=0.013961837626993656, Train=1.0, Val=0.77, Test=0.803\n",
      "Epoch=1419, Loss=0.02943888120353222, Train=1.0, Val=0.772, Test=0.803\n",
      "Epoch=1420, Loss=0.022952524945139885, Train=1.0, Val=0.774, Test=0.803\n",
      "Epoch=1421, Loss=0.023432642221450806, Train=1.0, Val=0.774, Test=0.803\n",
      "Epoch=1422, Loss=0.009625174105167389, Train=1.0, Val=0.776, Test=0.803\n",
      "Epoch=1423, Loss=0.012002754025161266, Train=1.0, Val=0.778, Test=0.803\n",
      "Epoch=1424, Loss=0.011449616402387619, Train=1.0, Val=0.784, Test=0.803\n",
      "Epoch=1425, Loss=0.008534415625035763, Train=1.0, Val=0.784, Test=0.803\n",
      "Epoch=1426, Loss=0.006718880496919155, Train=1.0, Val=0.788, Test=0.803\n",
      "Epoch=1427, Loss=0.018842091783881187, Train=1.0, Val=0.786, Test=0.803\n",
      "Epoch=1428, Loss=0.01202400866895914, Train=1.0, Val=0.792, Test=0.803\n",
      "Epoch=1429, Loss=0.030983546748757362, Train=1.0, Val=0.794, Test=0.807\n",
      "Epoch=1430, Loss=0.009608996100723743, Train=1.0, Val=0.792, Test=0.807\n",
      "Epoch=1431, Loss=0.009134725667536259, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1432, Loss=0.018450090661644936, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1433, Loss=0.019203314557671547, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1434, Loss=0.004074548836797476, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1435, Loss=0.017897363752126694, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1436, Loss=0.030044477432966232, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1437, Loss=0.02851514331996441, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1438, Loss=0.022052744403481483, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1439, Loss=0.01048788521438837, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1440, Loss=0.013181890361011028, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1441, Loss=0.049199219793081284, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1442, Loss=0.010507380589842796, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1443, Loss=0.006882862653583288, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1444, Loss=0.016395756974816322, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1445, Loss=0.011695725843310356, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=1446, Loss=0.024422237649559975, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1447, Loss=0.03161882609128952, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1448, Loss=0.018798798322677612, Train=1.0, Val=0.792, Test=0.807\n",
      "Epoch=1449, Loss=0.019315054640173912, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1450, Loss=0.005367625970393419, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1451, Loss=0.021282566711306572, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1452, Loss=0.008086656220257282, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1453, Loss=0.006810523569583893, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=1454, Loss=0.01257143635302782, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=1455, Loss=0.01721402257680893, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1456, Loss=0.040281254798173904, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1457, Loss=0.03183537721633911, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1458, Loss=0.008295970968902111, Train=1.0, Val=0.792, Test=0.807\n",
      "Epoch=1459, Loss=0.01185335498303175, Train=1.0, Val=0.794, Test=0.807\n",
      "Epoch=1460, Loss=0.012048785574734211, Train=1.0, Val=0.79, Test=0.807\n",
      "Epoch=1461, Loss=0.013646013103425503, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1462, Loss=0.00983502995222807, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1463, Loss=0.01401667483150959, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1464, Loss=0.03140712156891823, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1465, Loss=0.04927598312497139, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=1466, Loss=0.013109414838254452, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1467, Loss=0.006640652660280466, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1468, Loss=0.01608201488852501, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1469, Loss=0.06843427568674088, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1470, Loss=0.006435214541852474, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1471, Loss=0.008812984451651573, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1472, Loss=0.013940098695456982, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1473, Loss=0.03246459364891052, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1474, Loss=0.012171664275228977, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1475, Loss=0.04344915226101875, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=1476, Loss=0.008702918887138367, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1477, Loss=0.006652402225881815, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1478, Loss=0.005907987244427204, Train=1.0, Val=0.788, Test=0.807\n",
      "Epoch=1479, Loss=0.010013354010879993, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1480, Loss=0.035820864140987396, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1481, Loss=0.012841644696891308, Train=1.0, Val=0.784, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1482, Loss=0.022638825699687004, Train=1.0, Val=0.786, Test=0.807\n",
      "Epoch=1483, Loss=0.022831030189990997, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1484, Loss=0.014879448339343071, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1485, Loss=0.03966489061713219, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1486, Loss=0.026540514081716537, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1487, Loss=0.012105941772460938, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1488, Loss=0.014329951256513596, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1489, Loss=0.016919663175940514, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1490, Loss=0.029570722952485085, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1491, Loss=0.005104699172079563, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1492, Loss=0.017845643684267998, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1493, Loss=0.014155266806483269, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1494, Loss=0.009632227011024952, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1495, Loss=0.013646099716424942, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1496, Loss=0.02835819311439991, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1497, Loss=0.01157377939671278, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1498, Loss=0.017422160133719444, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1499, Loss=0.00614388519898057, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1500, Loss=0.019808795303106308, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1501, Loss=0.016450295224785805, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1502, Loss=0.004369370173662901, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1503, Loss=0.009368984960019588, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1504, Loss=0.023268193006515503, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1505, Loss=0.0068686106242239475, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1506, Loss=0.02133961208164692, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1507, Loss=0.014327729120850563, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=1508, Loss=0.014355385676026344, Train=1.0, Val=0.784, Test=0.807\n",
      "Epoch=1509, Loss=0.014667696319520473, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1510, Loss=0.005805063992738724, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1511, Loss=0.05064653232693672, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1512, Loss=0.009256764315068722, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1513, Loss=0.024740668013691902, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1514, Loss=0.042649660259485245, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1515, Loss=0.031201761215925217, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1516, Loss=0.03009849227964878, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1517, Loss=0.007469178177416325, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1518, Loss=0.006614593788981438, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1519, Loss=0.01109582930803299, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1520, Loss=0.03495660051703453, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1521, Loss=0.01053753960877657, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1522, Loss=0.01985178329050541, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1523, Loss=0.014744265004992485, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1524, Loss=0.02137761376798153, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1525, Loss=0.01906021498143673, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1526, Loss=0.018708061426877975, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1527, Loss=0.011872698552906513, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1528, Loss=0.010517438873648643, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1529, Loss=0.017857525497674942, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1530, Loss=0.012762521393597126, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1531, Loss=0.008710851892828941, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1532, Loss=0.044357333332300186, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1533, Loss=0.011678420007228851, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1534, Loss=0.016710923984646797, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1535, Loss=0.012813916429877281, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1536, Loss=0.009326876141130924, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1537, Loss=0.008867315948009491, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1538, Loss=0.013199906796216965, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1539, Loss=0.0041098082438111305, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1540, Loss=0.03174089267849922, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1541, Loss=0.02265295572578907, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1542, Loss=0.009561872109770775, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1543, Loss=0.007563989609479904, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1544, Loss=0.029223840683698654, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1545, Loss=0.0219869464635849, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1546, Loss=0.026356449350714684, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1547, Loss=0.013932660222053528, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1548, Loss=0.031078031286597252, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1549, Loss=0.023675398901104927, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1550, Loss=0.004335329402238131, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1551, Loss=0.01258492935448885, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1552, Loss=0.010815356858074665, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1553, Loss=0.015422535128891468, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1554, Loss=0.01704617217183113, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1555, Loss=0.037089258432388306, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1556, Loss=0.01175076887011528, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1557, Loss=0.04724772274494171, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1558, Loss=0.021890439093112946, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1559, Loss=0.005406241398304701, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1560, Loss=0.016307445243000984, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1561, Loss=0.01578214392066002, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1562, Loss=0.016240406781435013, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1563, Loss=0.025063063949346542, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1564, Loss=0.01410869974642992, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1565, Loss=0.010573156177997589, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1566, Loss=0.05861135944724083, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1567, Loss=0.005915910936892033, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1568, Loss=0.019782766699790955, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1569, Loss=0.0034550740383565426, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1570, Loss=0.022046832367777824, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1571, Loss=0.014513843692839146, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1572, Loss=0.04168309271335602, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1573, Loss=0.013053597882390022, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1574, Loss=0.03677888214588165, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1575, Loss=0.008747778832912445, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1576, Loss=0.014491151086986065, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1577, Loss=0.03819837421178818, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1578, Loss=0.012939805164933205, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1579, Loss=0.009750314056873322, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1580, Loss=0.010698758997023106, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1581, Loss=0.01650543324649334, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1582, Loss=0.034033842384815216, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1583, Loss=0.027224531397223473, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1584, Loss=0.03342457488179207, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1585, Loss=0.022175874561071396, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1586, Loss=0.005858235526829958, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1587, Loss=0.008803234435617924, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1588, Loss=0.024079661816358566, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1589, Loss=0.010099681094288826, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1590, Loss=0.009884069673717022, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1591, Loss=0.04066087678074837, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1592, Loss=0.00958514679223299, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1593, Loss=0.01190093718469143, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1594, Loss=0.017477912828326225, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1595, Loss=0.010181427001953125, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1596, Loss=0.009883689694106579, Train=1.0, Val=0.758, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1597, Loss=0.056023478507995605, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1598, Loss=0.008570109494030476, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1599, Loss=0.055782217532396317, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1600, Loss=0.01673431135714054, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1601, Loss=0.039659686386585236, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1602, Loss=0.008403000421822071, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1603, Loss=0.008468271233141422, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1604, Loss=0.025915050879120827, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1605, Loss=0.02224881947040558, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1606, Loss=0.0138250682502985, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1607, Loss=0.024395275861024857, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1608, Loss=0.019353974610567093, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1609, Loss=0.010971475392580032, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1610, Loss=0.018177473917603493, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1611, Loss=0.022822018712759018, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1612, Loss=0.017802966758608818, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1613, Loss=0.024708217009902, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1614, Loss=0.015805870294570923, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1615, Loss=0.010983477346599102, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1616, Loss=0.012883467599749565, Train=1.0, Val=0.782, Test=0.807\n",
      "Epoch=1617, Loss=0.01175564993172884, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=1618, Loss=0.011832918040454388, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1619, Loss=0.007315685041248798, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1620, Loss=0.024165434762835503, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1621, Loss=0.0019098512129858136, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1622, Loss=0.024579690769314766, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1623, Loss=0.006637044250965118, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1624, Loss=0.013042674399912357, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1625, Loss=0.014094275422394276, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1626, Loss=0.0076626138761639595, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1627, Loss=0.029818782582879066, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1628, Loss=0.020295899361371994, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1629, Loss=0.020314466208219528, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1630, Loss=0.027317829430103302, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1631, Loss=0.025714052841067314, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1632, Loss=0.02381649613380432, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1633, Loss=0.009191461838781834, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1634, Loss=0.022121144458651543, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1635, Loss=0.013056593015789986, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1636, Loss=0.010795497335493565, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1637, Loss=0.02756001614034176, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1638, Loss=0.012646753340959549, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1639, Loss=0.03086894564330578, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1640, Loss=0.01575194112956524, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1641, Loss=0.01631258986890316, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1642, Loss=0.03251982107758522, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1643, Loss=0.010585458017885685, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1644, Loss=0.019073475152254105, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1645, Loss=0.01536635123193264, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1646, Loss=0.023640289902687073, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1647, Loss=0.014033359475433826, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1648, Loss=0.02129512093961239, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1649, Loss=0.015347998589277267, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1650, Loss=0.00834690686315298, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1651, Loss=0.015077182091772556, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1652, Loss=0.010783739387989044, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1653, Loss=0.01074984110891819, Train=1.0, Val=0.74, Test=0.807\n",
      "Epoch=1654, Loss=0.006204234901815653, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1655, Loss=0.03974549472332001, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1656, Loss=0.00706816092133522, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1657, Loss=0.0098753422498703, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1658, Loss=0.05321235954761505, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1659, Loss=0.020352400839328766, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1660, Loss=0.021238012239336967, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1661, Loss=0.02237294614315033, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1662, Loss=0.01914267987012863, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1663, Loss=0.008900403045117855, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1664, Loss=0.010974620468914509, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1665, Loss=0.015032792463898659, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1666, Loss=0.004599201958626509, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1667, Loss=0.0072985803708434105, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1668, Loss=0.05920059233903885, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1669, Loss=0.02552984282374382, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1670, Loss=0.05654817074537277, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1671, Loss=0.011050806380808353, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1672, Loss=0.015465850941836834, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1673, Loss=0.0069387806579470634, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1674, Loss=0.024735234677791595, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1675, Loss=0.01806952990591526, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1676, Loss=0.011215637437999249, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1677, Loss=0.016917765140533447, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1678, Loss=0.03563686087727547, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1679, Loss=0.011462134309113026, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1680, Loss=0.018353909254074097, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1681, Loss=0.031808897852897644, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1682, Loss=0.015063774771988392, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1683, Loss=0.016813818365335464, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1684, Loss=0.016218993812799454, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1685, Loss=0.015562011860311031, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1686, Loss=0.0359966866672039, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1687, Loss=0.017529895529150963, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1688, Loss=0.008536783047020435, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1689, Loss=0.008168276399374008, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1690, Loss=0.012927936390042305, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1691, Loss=0.015958191826939583, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1692, Loss=0.01733686961233616, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1693, Loss=0.02562904916703701, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1694, Loss=0.010775606147944927, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1695, Loss=0.029051825404167175, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1696, Loss=0.012665345333516598, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1697, Loss=0.0021925736218690872, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1698, Loss=0.002947678091004491, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1699, Loss=0.010839314199984074, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1700, Loss=0.03761326149106026, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1701, Loss=0.004476009868085384, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1702, Loss=0.003816015785560012, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1703, Loss=0.003728611161932349, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1704, Loss=0.009759383276104927, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1705, Loss=0.03535529971122742, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1706, Loss=0.016276471316814423, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1707, Loss=0.0043866196647286415, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1708, Loss=0.024168454110622406, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1709, Loss=0.015108609572052956, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1710, Loss=0.01264670118689537, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1711, Loss=0.02640077844262123, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1712, Loss=0.013015060685575008, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1713, Loss=0.006599460728466511, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1714, Loss=0.022606326267123222, Train=1.0, Val=0.776, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1715, Loss=0.007886392995715141, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1716, Loss=0.019019488245248795, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1717, Loss=0.04145025461912155, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1718, Loss=0.01692551001906395, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1719, Loss=0.006539784837514162, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1720, Loss=0.08293482661247253, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1721, Loss=0.04756828397512436, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1722, Loss=0.003659726120531559, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1723, Loss=0.0063128843903541565, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1724, Loss=0.01261887140572071, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1725, Loss=0.023569244891405106, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1726, Loss=0.01063874363899231, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1727, Loss=0.03411957249045372, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1728, Loss=0.012071045115590096, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1729, Loss=0.016501612961292267, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1730, Loss=0.015771936625242233, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1731, Loss=0.015533997677266598, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1732, Loss=0.029851427301764488, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1733, Loss=0.008487408980727196, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1734, Loss=0.009012900292873383, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1735, Loss=0.018360134214162827, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1736, Loss=0.02471647597849369, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1737, Loss=0.01626344583928585, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1738, Loss=0.039792899042367935, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1739, Loss=0.007655602879822254, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1740, Loss=0.019197477027773857, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1741, Loss=0.014634847640991211, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1742, Loss=0.022123733535408974, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1743, Loss=0.022762177512049675, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1744, Loss=0.009579461999237537, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1745, Loss=0.02294192649424076, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1746, Loss=0.015430254861712456, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1747, Loss=0.01786366105079651, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1748, Loss=0.0061144037172198296, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1749, Loss=0.01897336170077324, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1750, Loss=0.012267653830349445, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1751, Loss=0.011574629694223404, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1752, Loss=0.03699444606900215, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1753, Loss=0.04997367039322853, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1754, Loss=0.0034993449226021767, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1755, Loss=0.02404240518808365, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1756, Loss=0.014211811125278473, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1757, Loss=0.010047019459307194, Train=1.0, Val=0.736, Test=0.807\n",
      "Epoch=1758, Loss=0.026477685198187828, Train=1.0, Val=0.736, Test=0.807\n",
      "Epoch=1759, Loss=0.023305701091885567, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1760, Loss=0.011183617636561394, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1761, Loss=0.03670659288764, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1762, Loss=0.010405763052403927, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1763, Loss=0.022222060710191727, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1764, Loss=0.02220449410378933, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1765, Loss=0.0037205182015895844, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1766, Loss=0.027283938601613045, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1767, Loss=0.011354917660355568, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1768, Loss=0.011828332208096981, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1769, Loss=0.006232583429664373, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1770, Loss=0.03527111932635307, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1771, Loss=0.011408212594687939, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1772, Loss=0.02267508953809738, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1773, Loss=0.02381870523095131, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1774, Loss=0.01736011542379856, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1775, Loss=0.012384561821818352, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1776, Loss=0.007810373790562153, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1777, Loss=0.026715058833360672, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1778, Loss=0.035234369337558746, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1779, Loss=0.03861120343208313, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1780, Loss=0.024661922827363014, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1781, Loss=0.004256235435605049, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1782, Loss=0.021149201318621635, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1783, Loss=0.012793602421879768, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1784, Loss=0.024953220039606094, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1785, Loss=0.005735182203352451, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1786, Loss=0.007452594581991434, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1787, Loss=0.032915182411670685, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1788, Loss=0.024319957941770554, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1789, Loss=0.015273967757821083, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1790, Loss=0.019541841000318527, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1791, Loss=0.006554150488227606, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1792, Loss=0.04618610814213753, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1793, Loss=0.012506568804383278, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1794, Loss=0.01788339763879776, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1795, Loss=0.007920428179204464, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1796, Loss=0.009983022697269917, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1797, Loss=0.052404262125492096, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1798, Loss=0.01505853608250618, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1799, Loss=0.026262812316417694, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1800, Loss=0.020494837313890457, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1801, Loss=0.02193930558860302, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1802, Loss=0.013808298856019974, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1803, Loss=0.01470141764730215, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1804, Loss=0.02053794451057911, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1805, Loss=0.03039463981986046, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1806, Loss=0.02043634094297886, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1807, Loss=0.01474621333181858, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1808, Loss=0.015364643186330795, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1809, Loss=0.039089225232601166, Train=1.0, Val=0.734, Test=0.807\n",
      "Epoch=1810, Loss=0.020326972007751465, Train=1.0, Val=0.738, Test=0.807\n",
      "Epoch=1811, Loss=0.018905723467469215, Train=1.0, Val=0.738, Test=0.807\n",
      "Epoch=1812, Loss=0.004698994569480419, Train=1.0, Val=0.738, Test=0.807\n",
      "Epoch=1813, Loss=0.018594635650515556, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1814, Loss=0.012844977900385857, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1815, Loss=0.013317066244781017, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1816, Loss=0.017477819696068764, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1817, Loss=0.006826989818364382, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1818, Loss=0.008648042567074299, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1819, Loss=0.034969229251146317, Train=1.0, Val=0.77, Test=0.807\n",
      "Epoch=1820, Loss=0.015464096330106258, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1821, Loss=0.010455028153955936, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1822, Loss=0.060052018612623215, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1823, Loss=0.007267999462783337, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1824, Loss=0.01942250318825245, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1825, Loss=0.00929822213947773, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1826, Loss=0.01655065082013607, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1827, Loss=0.013793355785310268, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1828, Loss=0.005562765058130026, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1829, Loss=0.015289238654077053, Train=1.0, Val=0.754, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1830, Loss=0.01765117608010769, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1831, Loss=0.009110425598919392, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1832, Loss=0.028500191867351532, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1833, Loss=0.015194864012300968, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1834, Loss=0.015915149822831154, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1835, Loss=0.010195822454988956, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1836, Loss=0.01903308555483818, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1837, Loss=0.014159154146909714, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1838, Loss=0.027290649712085724, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1839, Loss=0.009686962701380253, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1840, Loss=0.030918052420020103, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1841, Loss=0.006996823940426111, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1842, Loss=0.014999529346823692, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1843, Loss=0.015463088639080524, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1844, Loss=0.010532897897064686, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1845, Loss=0.01945476047694683, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1846, Loss=0.013918980956077576, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1847, Loss=0.010744679719209671, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1848, Loss=0.024495501071214676, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1849, Loss=0.01360259298235178, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1850, Loss=0.0035512540489435196, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1851, Loss=0.04238202050328255, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1852, Loss=0.0053674704395234585, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1853, Loss=0.020700767636299133, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1854, Loss=0.006492976564913988, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1855, Loss=0.01368617545813322, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1856, Loss=0.02175433561205864, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1857, Loss=0.013011462055146694, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1858, Loss=0.013140803202986717, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1859, Loss=0.007969416677951813, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1860, Loss=0.012226028367877007, Train=1.0, Val=0.734, Test=0.807\n",
      "Epoch=1861, Loss=0.01448522787541151, Train=1.0, Val=0.736, Test=0.807\n",
      "Epoch=1862, Loss=0.01790594309568405, Train=1.0, Val=0.738, Test=0.807\n",
      "Epoch=1863, Loss=0.01487589068710804, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1864, Loss=0.023586628958582878, Train=1.0, Val=0.742, Test=0.807\n",
      "Epoch=1865, Loss=0.011856930330395699, Train=1.0, Val=0.74, Test=0.807\n",
      "Epoch=1866, Loss=0.038495369255542755, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1867, Loss=0.016910376027226448, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1868, Loss=0.005830604583024979, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1869, Loss=0.014054558239877224, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1870, Loss=0.004672002978622913, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1871, Loss=0.013954227790236473, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1872, Loss=0.006900487467646599, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1873, Loss=0.005504060536623001, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1874, Loss=0.006110016256570816, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1875, Loss=0.008948754519224167, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1876, Loss=0.0045742555521428585, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1877, Loss=0.013863141648471355, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1878, Loss=0.010183097794651985, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1879, Loss=0.009891558438539505, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1880, Loss=0.006956815253943205, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1881, Loss=0.00279101449996233, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1882, Loss=0.018573548644781113, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1883, Loss=0.009683767333626747, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1884, Loss=0.018509170040488243, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1885, Loss=0.007268677465617657, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1886, Loss=0.006165610160678625, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1887, Loss=0.02208670787513256, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1888, Loss=0.019746633246541023, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1889, Loss=0.01899854838848114, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1890, Loss=0.013601276092231274, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1891, Loss=0.005177603103220463, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1892, Loss=0.023051097989082336, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1893, Loss=0.008986365981400013, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1894, Loss=0.0057549672201275826, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1895, Loss=0.010667987167835236, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1896, Loss=0.017756639048457146, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1897, Loss=0.043540675193071365, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1898, Loss=0.02134152315557003, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1899, Loss=0.024419615045189857, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1900, Loss=0.016310520470142365, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1901, Loss=0.01376628503203392, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1902, Loss=0.04429105669260025, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1903, Loss=0.018123259767889977, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1904, Loss=0.01798934116959572, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1905, Loss=0.014584851451218128, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1906, Loss=0.009424234740436077, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1907, Loss=0.006441161502152681, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1908, Loss=0.0401727594435215, Train=1.0, Val=0.778, Test=0.807\n",
      "Epoch=1909, Loss=0.025387877598404884, Train=1.0, Val=0.774, Test=0.807\n",
      "Epoch=1910, Loss=0.011257356032729149, Train=1.0, Val=0.776, Test=0.807\n",
      "Epoch=1911, Loss=0.021930662915110588, Train=1.0, Val=0.78, Test=0.807\n",
      "Epoch=1912, Loss=0.01996682956814766, Train=1.0, Val=0.772, Test=0.807\n",
      "Epoch=1913, Loss=0.012916951440274715, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1914, Loss=0.022708339616656303, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1915, Loss=0.02182510495185852, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1916, Loss=0.02769153192639351, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1917, Loss=0.022536402568221092, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1918, Loss=0.03207989037036896, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1919, Loss=0.011662001721560955, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1920, Loss=0.017264297232031822, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1921, Loss=0.014961753971874714, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1922, Loss=0.008289785124361515, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1923, Loss=0.007574893534183502, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1924, Loss=0.03673972934484482, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1925, Loss=0.025681160390377045, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1926, Loss=0.026203053072094917, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1927, Loss=0.01826871559023857, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1928, Loss=0.02986716292798519, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1929, Loss=0.01074276864528656, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1930, Loss=0.023383809253573418, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1931, Loss=0.015648625791072845, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1932, Loss=0.03179110959172249, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1933, Loss=0.018087025731801987, Train=1.0, Val=0.744, Test=0.807\n",
      "Epoch=1934, Loss=0.039608296006917953, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1935, Loss=0.06566862016916275, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1936, Loss=0.012331829406321049, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1937, Loss=0.012485559098422527, Train=1.0, Val=0.768, Test=0.807\n",
      "Epoch=1938, Loss=0.011901850812137127, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1939, Loss=0.01805482991039753, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1940, Loss=0.030791722238063812, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1941, Loss=0.040790215134620667, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1942, Loss=0.011665194295346737, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1943, Loss=0.01683400571346283, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1944, Loss=0.02593032270669937, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1945, Loss=0.019986996427178383, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1946, Loss=0.036765728145837784, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1947, Loss=0.013575448654592037, Train=1.0, Val=0.752, Test=0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1948, Loss=0.026012737303972244, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1949, Loss=0.014268307946622372, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1950, Loss=0.00740260723978281, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1951, Loss=0.01484525203704834, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1952, Loss=0.015204086899757385, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1953, Loss=0.009975377470254898, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1954, Loss=0.03125984966754913, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1955, Loss=0.005742434877902269, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1956, Loss=0.013767356984317303, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1957, Loss=0.0034489731770008802, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1958, Loss=0.014955191873013973, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1959, Loss=0.029695341363549232, Train=1.0, Val=0.766, Test=0.807\n",
      "Epoch=1960, Loss=0.007055478636175394, Train=1.0, Val=0.762, Test=0.807\n",
      "Epoch=1961, Loss=0.006072315853089094, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1962, Loss=0.010145296342670918, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1963, Loss=0.023084085434675217, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1964, Loss=0.010958047583699226, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1965, Loss=0.021669898182153702, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1966, Loss=0.016447076573967934, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1967, Loss=0.01989113911986351, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1968, Loss=0.02184920385479927, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1969, Loss=0.008702825754880905, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1970, Loss=0.016471780836582184, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1971, Loss=0.02653077058494091, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1972, Loss=0.018559707328677177, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1973, Loss=0.007958588190376759, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1974, Loss=0.025319406762719154, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1975, Loss=0.009495193138718605, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=1976, Loss=0.004946165718138218, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1977, Loss=0.008001726120710373, Train=1.0, Val=0.746, Test=0.807\n",
      "Epoch=1978, Loss=0.011120307259261608, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1979, Loss=0.02064843662083149, Train=1.0, Val=0.75, Test=0.807\n",
      "Epoch=1980, Loss=0.005582163110375404, Train=1.0, Val=0.748, Test=0.807\n",
      "Epoch=1981, Loss=0.012354041449725628, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1982, Loss=0.014309278689324856, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1983, Loss=0.0066772098653018475, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1984, Loss=0.006179255899041891, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1985, Loss=0.008724924176931381, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1986, Loss=0.04215288162231445, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1987, Loss=0.013519534841179848, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1988, Loss=0.0061253453604876995, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1989, Loss=0.03029453381896019, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1990, Loss=0.01837158016860485, Train=1.0, Val=0.76, Test=0.807\n",
      "Epoch=1991, Loss=0.005451579578220844, Train=1.0, Val=0.764, Test=0.807\n",
      "Epoch=1992, Loss=0.05901048704981804, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1993, Loss=0.012432207353413105, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1994, Loss=0.01183522678911686, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1995, Loss=0.019454458728432655, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1996, Loss=0.011672432534396648, Train=1.0, Val=0.758, Test=0.807\n",
      "Epoch=1997, Loss=0.010988160967826843, Train=1.0, Val=0.754, Test=0.807\n",
      "Epoch=1998, Loss=0.011266086250543594, Train=1.0, Val=0.756, Test=0.807\n",
      "Epoch=1999, Loss=0.008328341878950596, Train=1.0, Val=0.752, Test=0.807\n",
      "Epoch=2000, Loss=0.023168375715613365, Train=1.0, Val=0.754, Test=0.807\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = final_test_acc = 0\n",
    "epochs=2000\n",
    "for epoch in range(0, epochs):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f\"Epoch={epoch+1}, Loss={loss}, Train={train_acc}, Val={val_acc}, Test={test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f556a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
